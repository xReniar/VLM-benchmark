{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7375c62e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd367240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainer/anaconda3/envs/vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rainer/Code/VLM-benchmark')\n",
    "\n",
    "from dataset import *\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d58e0",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b49bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in extracting visual data. \n",
    "Your task is to process and extract meaningful insights from images, \n",
    "leveraging multimodal understanding to provide accurate and contextually relevant information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json2xml import json2xml\n",
    "from json2xml.utils import readfromstring\n",
    "from lxml import etree\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def image_to_base64(pil_image):\n",
    "    buf = BytesIO()\n",
    "    pil_image.save(buf, format=\"JPEG\")\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def format_data(sample, train_type: str):\n",
    "    pil_image = Image.open(sample.image_path)\n",
    "\n",
    "    field_names = set([entity.label for entity in sample.entities])\n",
    "    if train_type == \"xml\":\n",
    "        xml_fields = \"\".join([f\"<{field}>..</{field}>\" for field in field_names])\n",
    "        output_format = f\"<kie>{xml_fields}</kie>\"\n",
    "        prompt = \"Extract the following {fields} from the above document. If a field is not present, return ''. Return the output in a valid XML format like {output_format}\" \\\n",
    "            .format(\n",
    "                fields = list(field_names),\n",
    "                output_format = output_format\n",
    "            )\n",
    "    else:\n",
    "        output_format = {field: \"..\" for field in field_names}\n",
    "\n",
    "        prompt = \"Extract the following {fields} from the above document. If a field is not present, return ''. Return the output in a valid JSON format like {output_format}\" \\\n",
    "            .format(\n",
    "                fields = list(field_names),\n",
    "                output_train_type = \"normal\"\n",
    "\n",
    "train_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"train\")]\n",
    "test_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "\n",
    "train_dataset[0]format = output_format\n",
    "            )\n",
    "\n",
    "    if train_type == \"normal\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image },\n",
    "                    { \"type\": \"text\", \"text\": prompt }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": sample.to_json(\"kie\")\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    elif train_type == \"no-prompt\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": sample.to_json(\"kie\")\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    elif train_type == \"xml\":\n",
    "        label = json2xml.Json2xml(\n",
    "            data=readfromstring(json.dumps(sample.to_json(\"kie\"))),\n",
    "            wrapper=\"kie\",\n",
    "            pretty=False,\n",
    "            attr_type=False\n",
    "        ).to_xml()\n",
    "        label = etree.tostring(\n",
    "            etree.fromstring(label),\n",
    "            encoding=\"unicode\",\n",
    "            pretty_print=False\n",
    "        )\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image },\n",
    "                    { \"type\": \"text\", \"text\": prompt }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": label\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        raise Exception(f\"{train_type} value error\")\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6073e20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'You are a highly advanced Vision Language Model (VLM), specialized in extracting visual data. \\nYour task is to process and extract meaningful insights from images, \\nleveraging multimodal understanding to provide accurate and contextually relevant information.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=463x1013>},\n",
       "   {'type': 'text',\n",
       "    'text': \"Extract the following ['date', 'address', 'total', 'company'] from the above document. If a field is not present, return ''. Return the output in a valid XML format like <kie><date>..</date><address>..</address><total>..</total><company>..</company></kie>\"}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': '<kie><company>BOOK TA .K (TAMAN DAYA) SDN BHD</company><date>25/12/2018</date><address>NO.53 55,57 &amp; 59, JALAN SAGU 18, TAMAN DAYA, 81100 JOHOR BAHRU, JOHOR.</address><total>9.00</total></kie>'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_type = \"normal\"\n",
    "\n",
    "train_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"train\")]\n",
    "test_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2eb387",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "import torch\n",
    "from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"down_proj\", \"o_proj\", \"k_proj\", \"q_proj\", \"gate_proj\", \"up_proj\", \"v_proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "    processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "]\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "\n",
    "    image_inputs = []\n",
    "    for example in examples:\n",
    "        image = example[1][\"content\"][0][\"image\"]\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image_inputs.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "    labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"smolvlm-instruct-trl-sft-sroie\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50fe4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
