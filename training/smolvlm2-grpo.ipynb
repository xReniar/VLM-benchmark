{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5e44cf",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from dataset import *\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForImageTextToText\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from Levenshtein import distance as edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc080f4c",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9001887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bf4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in extracting visual data.\n",
    "Your task is to extract from the image the requested fields and return the result only in a valid JSON format.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8327a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_fn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    imgs_fn.append(sample.image_path.split(\"/\")[-1])\n",
    "    pil_image = Image.open(sample.image_path)\n",
    "\n",
    "    field_names = set(sorted([entity.label for entity in sample.entities]))\n",
    "    output_format = json.dumps({field: \"..\" for field in field_names})\n",
    "\n",
    "    prompt = \"Extract the following {fields} from the above document. \\\n",
    "              If a field is not present, return ''. Return the output in a valid JSON format like {output_format}. \\\n",
    "              For date fields make sure to return date in this format dd/mm/yyyy\". \\\n",
    "        .format(\n",
    "            fields = list(field_names),\n",
    "            output_format = output_format\n",
    "        )\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_message}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"image\": pil_image,\n",
    "        \"answer\": json.dumps(sample.to_json(\"kie\"))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea6e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'image', 'answer'],\n",
      "    num_rows: 626\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [format_data(sample) for sample in SROIE(tasks=[\"kie\"], split=\"train\")]\n",
    "test_dataset = [format_data(sample) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79a8ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>System: <end_of_utterance>\n",
      "User:<image>Extract the following ['company', 'address', 'total', 'date'] from the image<end_of_utterance>\n",
      "Assistant:\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=463x1013 at 0x7C4B68B265D0>\n",
      "{\"company\": \"BOOK TA .K (TAMAN DAYA) SDN BHD\", \"date\": \"25/12/2018\", \"address\": \"NO.53 55,57 & 59, JALAN SAGU 18, TAMAN DAYA, 81100 JOHOR BAHRU, JOHOR.\", \"total\": \"9.00\"}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"prompt\"])\n",
    "print(train_dataset[0][\"image\"])\n",
    "print(train_dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d4e0c",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    _attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030fac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model = model,\n",
    "    model_id = \"final\",\n",
    "    is_trainable = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afae6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    #init_lora_weights=\"gaussian\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18480dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"result/smolvlm2-grpo\",\n",
    "    learning_rate=2e-6,\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    #num_train_epochs=1,\n",
    "    max_steps=800,\n",
    "    bf16=True,\n",
    "    # Parameters that control the data preprocessing\n",
    "    per_device_train_batch_size=1,\n",
    "    max_completion_length=3000,  # default: 256\n",
    "    num_generations=4,  # default: 8\n",
    "    generation_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_prompt_length=None,\n",
    "    # Parameters related to reporting and saving\n",
    "    report_to=\"none\",\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    use_liger_loss=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_parsable(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion for completion in completions]\n",
    "    results = []\n",
    "    for response in responses:\n",
    "        try:\n",
    "            parsed = json.loads(re.search(r\"```json\\n\\s*(.*?)\\s*\\n```\", response, re.DOTALL).group(1).strip())\n",
    "            if isinstance(parsed, dict):\n",
    "                results.append(4.0)\n",
    "            else:\n",
    "                results.append(-4.0)\n",
    "        except:\n",
    "            results.append(-4.0)\n",
    "    print(\"json_parsable: \", results)\n",
    "    return results\n",
    "\n",
    "def all_fields_present(completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion for completion in completions]\n",
    "    results = []\n",
    "    for response, gt in zip(responses, answer):\n",
    "        labels = json.loads(gt).keys()\n",
    "        score = 0.0\n",
    "        try:\n",
    "            response_dict = json.loads(re.search(r\"```json\\n\\s*(.*?)\\s*\\n```\", response, re.DOTALL).group(1).strip())\n",
    "            if isinstance(response_dict, dict):\n",
    "                for label in list(labels):\n",
    "                    if label in response_dict.keys():\n",
    "                        score += 2.0\n",
    "                    else:\n",
    "                        score += -2.0\n",
    "\n",
    "                #results.append(2.0 if labels.issubset(response_dict.keys()) else -0.5)\n",
    "                results.append(score)\n",
    "            else:    \n",
    "                results.append(-4.0)\n",
    "        except:\n",
    "            results.append(-4.0)\n",
    "    print(\"fields_present: \", results)\n",
    "    return results\n",
    "\n",
    "def correct_labels(completions, answer, **kwargs) -> list[float]:\n",
    "    results = []\n",
    "    for completion, gt in zip(completions, answer):\n",
    "        score = 0.0\n",
    "        labels = json.loads(gt).keys()\n",
    "        try:\n",
    "            completion_dict = json.loads(re.search(r\"```json\\n\\s*(.*?)\\s*\\n```\", completion, re.DOTALL).group(1).strip())\n",
    "            gt_dict = json.loads(gt)\n",
    "            for label in labels:\n",
    "                pred = completion_dict.get(label, None)\n",
    "                gt_value = gt_dict.get(label, None)\n",
    "\n",
    "                if pred == None:\n",
    "                    score += -2.0\n",
    "                else:\n",
    "                    dist = edit_distance(pred, gt_value)\n",
    "                    if dist < 2:\n",
    "                        score += 2.0\n",
    "                    else:\n",
    "                        score += -dist\n",
    "                \n",
    "                '''\n",
    "                if pred == gt_value and pred is not None:\n",
    "                    score += 1.0\n",
    "                else:\n",
    "                    score -= 2.0\n",
    "                '''\n",
    "            results.append(score)\n",
    "        except:\n",
    "            results.append(-4.0)\n",
    "    print(\"corrent labels: \",results)\n",
    "    return results\n",
    "\n",
    "# TODO: aggiungere funzione per calcolare editdistance sui campi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6affc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    reward_funcs=[\n",
    "        json_parsable,\n",
    "        all_fields_present,\n",
    "        correct_labels\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{training_args.output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6945ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_fn = []\n",
    "\n",
    "test_dataset = [format_data(sample) for sample in DocILE(tasks=[\"kie\"], split=\"val\")]\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    _attn_implementation=\"eager\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model.load_adapter(f\"result-docile/smolvlm2-grpo/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8af4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for data, fn in zip(test_dataset, imgs_fn):\n",
    "    inputs = processor.apply_chat_template(\n",
    "        data[\"prompt\"],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "    start = time.time()\n",
    "    generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=1000)\n",
    "    end = time.time()\n",
    "\n",
    "    generated_texts = processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    results[fn] = dict(\n",
    "        response = generated_texts[0],\n",
    "        inference_time = end - start\n",
    "    )\n",
    "\n",
    "    with open(f\"result-docile/smolvlm2-grpo.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
