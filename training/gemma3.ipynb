{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139adc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from dataset import *\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "\n",
    "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainerCallback\n",
    "from unsloth import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bf788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json2xml import json2xml\n",
    "from json2xml.utils import readfromstring\n",
    "from lxml import etree\n",
    "import base64\n",
    "\n",
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in extracting visual data.\n",
    "Your task is to process and extract meaningful insights from images that are asked by the user.\"\"\"\n",
    "\n",
    "imgs_fn = []\n",
    "\n",
    "\n",
    "def format_data(sample, train_type: str):\n",
    "    pil_image = Image.open(sample.image_path)\n",
    "    imgs_fn.append(sample.image_path.split(\"/\")[-1])\n",
    "\n",
    "    field_names = set(sorted([entity.label for entity in sample.entities]))\n",
    "    if train_type == \"xml\":\n",
    "        xml_fields = \"\".join([f\"<{field}>..</{field}>\" for field in field_names])\n",
    "        output_format = f\"<kie>{xml_fields}</kie>\"\n",
    "        prompt = \"Extract the following {fields} from the above document. If a field is not present, return ''. Return the output in a valid XML format like {output_format}\" \\\n",
    "            .format(\n",
    "                fields = list(field_names),\n",
    "                output_format = output_format\n",
    "            )\n",
    "    else:\n",
    "        output_format = {field: \"..\" for field in field_names}\n",
    "\n",
    "        prompt = \"Extract the following {fields} from the above document. If a field is not present, return ''. Return the output in a valid JSON format like ```json\\n{output_format}\\n```\" \\\n",
    "            .format(\n",
    "                fields = list(field_names),\n",
    "                output_format = output_format\n",
    "            )\n",
    "\n",
    "    if train_type == \"normal\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image },\n",
    "                    { \"type\": \"text\", \"text\": prompt }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"```json\\n{json.dumps(sample.to_json('kie'))}\\n```\"\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    elif train_type == \"no-prompt\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"```json{json.dumps(sample.to_json('kie'))}```\"\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    elif train_type == \"xml\":\n",
    "        label = json2xml.Json2xml(\n",
    "            data=readfromstring(json.dumps(sample.to_json(\"kie\"))),\n",
    "            wrapper=\"kie\",\n",
    "            pretty=False,\n",
    "            attr_type=False\n",
    "        ).to_xml()\n",
    "        label = etree.tostring(\n",
    "            etree.fromstring(label),\n",
    "            encoding=\"unicode\",\n",
    "            pretty_print=False\n",
    "        )\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"image\", \"image\": pil_image },\n",
    "                    { \"type\": \"text\", \"text\": prompt }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": label\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        raise Exception(f\"{train_type} value error\")\n",
    "\n",
    "    return { \"messages\": conversation }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = \"normal\"\n",
    "dataset = \"docile\"\n",
    "\n",
    "if dataset == \"docile\":\n",
    "    train_dataset = [format_data(sample, train_type) for sample in DocILE(tasks=[\"kie\"], split=\"train\")]\n",
    "    test_dataset = [format_data(sample, train_type) for sample in DocILE(tasks=[\"kie\"], split=\"val\")]\n",
    "elif dataset == \"sroie\":\n",
    "    train_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"train\")]\n",
    "    test_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "elif dataset == \"nnts\":\n",
    "    test_dataset = [format_data(sample, train_type) for sample in NNTS_KIE(tasks=[\"kie\"], split=\"test\")]\n",
    "else:\n",
    "    raise Exception(\"Wrong dataset value\")\n",
    "\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3-4b-pt\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004836f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    #random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "import editdistance\n",
    "import xmltodict\n",
    "import re\n",
    "\n",
    "\n",
    "def compute_metrics(processor):\n",
    "    def inner_compute_metrics(eval_pred: EvalPrediction):\n",
    "        pred_ids, labels = eval_pred.predictions\n",
    "\n",
    "        pred_ids_list = pred_ids.tolist()\n",
    "        labels_list = labels.tolist()\n",
    "        \n",
    "        decoded_preds = []\n",
    "        decoded_labels = []\n",
    "        \n",
    "        for i in range(len(pred_ids_list)):\n",
    "            pred_tokens = [token_id for token_id in pred_ids_list[i] if token_id not in [-100, processor.tokenizer.pad_token_id]]\n",
    "            decoded_pred = processor.tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "            decoded_preds.append(decoded_pred)\n",
    "\n",
    "            label_tokens = [token_id for token_id in labels_list[i] if token_id not in [-100, processor.tokenizer.pad_token_id]]\n",
    "            decoded_label = processor.tokenizer.decode(label_tokens, skip_special_tokens=True)\n",
    "            decoded_labels.append(decoded_label)\n",
    "        \n",
    "        similarities = []\n",
    "        not_parsable = []\n",
    "        edit_distance = []\n",
    "        for i in range(len(decoded_preds)):\n",
    "            try:\n",
    "                #pred: dict = json.loads(decoded_preds[i].split('Assistant: ')[1])\n",
    "                #label: dict = json.loads(decoded_labels[i].split('Assistant: ')[1])\n",
    "                pred = decoded_preds[i].split('model')[1]\n",
    "                label = decoded_labels[i].split('model')[1]\n",
    "\n",
    "                pred = json.loads(re.search(r\"```json\\s*(.*?)\\s*```\", pred, re.DOTALL).group(1).strip())\n",
    "                label = json.loads(re.search(r\"```json\\s*(.*?)\\s*```\", label, re.DOTALL).group(1).strip())\n",
    "                #pred = xmltodict.parse(pred)[\"kie\"]\n",
    "                #label = xmltodict.parse(label)[\"kie\"]\n",
    "\n",
    "\n",
    "                field_sims = []\n",
    "                for k in label.keys():\n",
    "                    if k in pred:\n",
    "                        dist = editdistance.eval(str(pred[k]), str(label[k]))\n",
    "                        max_len = max(len(str(label[k])), 1)\n",
    "                        #max_len = max(len(str(pred[k])), len(str(label[k])), 1)\n",
    "                        sim = (1 - dist / max_len)\n",
    "                        field_sims.append(sim)\n",
    "                        edit_distance.append(sim)\n",
    "                    else:\n",
    "                        field_sims.append(0.0)\n",
    "\n",
    "                #print(\"similarity: \", sum(field_sims) / len(field_sims))\n",
    "                        \n",
    "                similarities.append(sum(field_sims) / len(field_sims))\n",
    "            except Exception as e:\n",
    "                similarities.append(0.0)\n",
    "                not_parsable.append(1.0)\n",
    "\n",
    "        #print(similarities)\n",
    "        #print(len(similarities))\n",
    "        #print(\"#\" * 100)\n",
    "\n",
    "        return {\n",
    "            \"Accuracy\": sum(similarities) / len(similarities),\n",
    "            \"Not Parsable\": int(sum(not_parsable)),\n",
    "            \"Edit Distance\": sum(edit_distance) / len(edit_distance) if len(edit_distance) != 0 else 0\n",
    "        }\n",
    "    return inner_compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ad7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits: tuple, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        with open(self.log_path, \"w\") as f:\n",
    "            json.dump(state.log_history, f, indent=4)\n",
    "        print(f\"Log saved in {self.log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d725e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    #warmup_ratio = 0.03,\n",
    "    max_grad_norm = 0.3,\n",
    "    warmup_steps=40,\n",
    "    max_steps = 180,\n",
    "    #num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "    learning_rate = 2e-3, # per docile 1e-3 per sroie 1e-4\n",
    "    logging_steps = 20,\n",
    "    save_steps=20,\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    #lr_scheduler_type=\"cosine\",\n",
    "    weight_decay = 0.01,\n",
    "    #seed = 3407,\n",
    "    output_dir = f\"result-{dataset}/gemma3_{train_type}\",\n",
    "    report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "    # You MUST put the below items for vision finetuning:\n",
    "    gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "    gradient_checkpointing = True,\n",
    "    remove_unused_columns = False,\n",
    "    dataset_text_field = \"\",\n",
    "    dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "    max_length = None,\n",
    "    ##\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    #metric_for_best_model=\"Accuracy\",\n",
    "    #label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = processor,\n",
    "    data_collator = UnslothVisionDataCollator(model, processor), # Must use!\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    args = training_args,\n",
    "    callbacks=[\n",
    "        JSONLoggerCallback(f\"log/gemma3-{train_type}-{dataset}.json\")\n",
    "    ],\n",
    "    #compute_metrics=compute_metrics(processor),\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9aaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a54e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"result-{dataset}/gemma3_{train_type}/lora_model\")\n",
    "processor.save_pretrained(f\"result-{dataset}/gemma3_{train_type}/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    model_name = f\"result-{dataset}/gemma3_{train_type}/lora_model\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_fn = []\n",
    "\n",
    "if dataset == \"docile\":\n",
    "    test_dataset = [format_data(sample, train_type) for sample in DocILE(tasks=[\"kie\"], split=\"val\")]\n",
    "elif dataset == \"sroie\":\n",
    "    test_dataset = [format_data(sample, train_type) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "elif dataset == \"nnts\":\n",
    "    test_dataset = [format_data(sample, train_type) for sample in NNTS_KIE(tasks=[\"kie\"], split=\"test\")]\n",
    "else:\n",
    "    raise Exception(\"Wrong dataset value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "os.makedirs(f\"result-{dataset}\", exist_ok=True)\n",
    "\n",
    "for data, fn in zip(test_dataset, imgs_fn):\n",
    "    new_data = data[\"messages\"][0:2]\n",
    "    img = new_data[1][\"content\"][0][\"image\"]\n",
    "\n",
    "    new_data[1][\"content\"][0] = {\"type\": \"image\"}\n",
    "    #text = new_data[1][\"content\"][1][\"text\"]\n",
    "    #new_data[1][\"content\"][1][\"text\"] = f\"<image>\\n{text}\"\n",
    "\n",
    "    input_text = processor.apply_chat_template(new_data, add_generation_prompt = True)\n",
    "    inputs = processor(\n",
    "        img,\n",
    "        input_text,\n",
    "        add_special_tokens = False,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    start = time.time()\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=3000,\n",
    "        use_cache=True,\n",
    "        temperature = 0.7, top_p = 0.95, top_k = 64, repetition_penalty=1.3\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    output_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    result[fn] = dict(\n",
    "        response = output_text,\n",
    "        t = end - start\n",
    "    )\n",
    "\n",
    "    with open(f\"result-{dataset}/gemma3n-notrain.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent = 4)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
