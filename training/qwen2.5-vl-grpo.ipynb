{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb2c2f9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from dataset import *\n",
    "from PIL import Image\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlmgrpo import VLMGRPOTrainer # YOU MUST IMPORT vlmgrpo before unsloth\n",
    "from trl import GRPOConfig\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth import is_bf16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in extracting visual data.\n",
    "Your task is to process and extract meaningful insights from images that are asked in the prompt.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279092a",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73435b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    pil_image = Image.open(sample.image_path)\n",
    "\n",
    "    field_names = set([entity.label for entity in sample.entities])\n",
    "\n",
    "    prompt = \"Extract the following {fields} from the image\" \\\n",
    "        .format(fields = list(field_names))\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": conversation,\n",
    "        \"image\": [pil_image],\n",
    "        \"answer\": json.dumps(sample.to_json(\"kie\"))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6615fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in SROIE(tasks=[\"kie\"], split=\"train\")]\n",
    "test_dataset = [format_data(sample) for sample in SROIE(tasks=[\"kie\"], split=\"test\")]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    bf16 = is_bf16_supported(),\n",
    "    fp16 = not is_bf16_supported(),\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e583323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_parsable(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    results = []\n",
    "    for response in responses:\n",
    "        try:\n",
    "            json.loads(response)\n",
    "            results.append(2.0)\n",
    "        except json.JSONDecodeError:\n",
    "            results.append(0.0)\n",
    "    return results\n",
    "\n",
    "def all_fields_present(completions, **kwargs) -> list[float]:\n",
    "    labels = {\"address\", \"date\", \"company\", \"total\"}\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    results = []\n",
    "    for response in responses:\n",
    "        try:\n",
    "            response_dict = json.loads(response)\n",
    "            results.append(2.0 if labels.issubset(response_dict.keys()) else 0.0)\n",
    "        except json.JSONDecodeError:\n",
    "            results.append(0.0)\n",
    "    return results\n",
    "\n",
    "def correct_labels(completions, answer, **kwargs) -> list[float]:\n",
    "    labels = {\"address\", \"date\", \"company\", \"total\"}\n",
    "    results = []\n",
    "    for completion, gt in zip(completions, answer):\n",
    "        score = 0.0\n",
    "        try:\n",
    "            completion_dict = json.loads(completion[0]['content'])\n",
    "            gt_dict = json.loads(gt)\n",
    "            for label in labels:\n",
    "                pred = completion_dict.get(label, None)\n",
    "                gt_value = gt_dict.get(label, None)\n",
    "\n",
    "                if pred == gt_value and pred is not None:\n",
    "                    score += 1.0\n",
    "                else:\n",
    "                    score -= 2.0\n",
    "            results.append(score)\n",
    "        except json.JSONDecodeError:\n",
    "            results.append(0.0)\n",
    "    return results\n",
    "\n",
    "# TODO: aggiungere funzione per calcolare editdistance sui campi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VLMGRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs = [\n",
    "        json_parsable,\n",
    "        all_fields_present,\n",
    "        correct_labels\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer, # MUST put unsloth processor here !\n",
    "    reward_processing_classes = tokenizer, #Here also\n",
    "    grad_verbose = True #Enable to monitor loss and grad during training \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448754aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
